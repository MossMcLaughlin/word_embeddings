{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3/2017 | Moss McLaughlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import itertools\n",
    "import time\n",
    "import operator\n",
    "import io\n",
    "import array\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START_TOKEN = \"sentence_start\"\n",
    "SENTENCE_END_TOKEN = \"sentence_end\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\"\n",
    "ARTICLE_END_TOKEN = \"</ARTICLE_END>\"\n",
    "NUM_TOKEN = \"<NUM>\"\n",
    "\n",
    "similarity_threshold = 0.95"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Pre-trained GloVe emeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load pre-trained GloVe word emeddings and build a dictionary.\n",
    "Dataset has vocabulary of 400,000.\n",
    "\n",
    "Example lines (500,501,502) of glove.6B.50d.txt (words are 50 dimensional vectors):\n",
    "\n",
    "college -1.2283 1.4176 -0.68625 -1.1615 -0.0019627 -0.52577 -1.5977 -0.25307 -0.21699 -0.56572 -0.14248 0.15765 0.18763 -1.3173 -0.53438 0.20573 -0.47985 1.0476 -0.5075 0.77326 0.39774 0.70346 -0.32232 0.60171 -0.13352 -1.842 -0.092598 -1.1288 -1.4859 -0.86235 2.5976 0.66146 -0.054094 -1.5755 1.1185 0.13911 -0.26915 0.57159 1.3328 -0.24861 -0.68554 -0.43956 -0.86744 0.95772 -0.67349 0.36049 0.61952 -0.26356 -0.3715 0.32093\n",
    "\n",
    "working 0.25792 -0.14413 -0.035634 -0.60551 0.11004 -0.058799 -1.2209 -0.031605 -0.023699 -0.37419 0.28924 0.12331 -0.31903 0.65017 0.28362 -0.20956 0.30423 0.75571 0.47964 -0.41976 0.68923 0.92026 0.070798 0.3948 0.24721 -1.4038 -0.14209 -0.6946 -0.035052 0.0041205 3.4024 0.036271 -0.58483 -0.72107 0.036996 0.33065 -0.27332 0.51897 0.3499 0.061199 -0.36178 -0.26534 0.4271 0.0081181 0.19844 -0.38564 -0.35535 0.032932 -0.50055 0.54358\n",
    "\n",
    "community 0.14774 0.62713 -0.81852 -0.16878 0.44055 -0.12515 -0.87369 -0.060084 0.48804 -0.23463 -0.017574 -0.70673 \n",
    "0.77392 -0.82772 -0.69719 0.076588 0.98306 0.58452 0.77501 0.30231 0.0059052 0.58345 -1.0178 1.1176 -0.13487 -1.2102 -0.095842 -0.70611 -0.54565 -0.12818 3.4346 0.12549 -0.35702 -1.3685 -0.32705 -0.25489 -0.52943 -0.90213 0.28179 0.22691 -0.47532 -0.51934 -0.32232 0.07629 -0.10132 0.47723 -0.83698 0.021588 -0.36972 0.14843\n",
    "\n",
    "The dataset is ordered from most to least frequent tokens.\n",
    "The three most frequent tokens are ['the'], [','], ['.'].  \n",
    "\n",
    "We can build a smaller dictionary to increase speed by only reading the first n lines of glove.6B.50d.txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "  print(\"Loading word embeddings...\")\n",
    "  with open('../word_embedding/glove.6B/glove.6B.50d.txt') as f:\n",
    "    words = {}\n",
    "    w = [line.split(' ') for line in f]\n",
    "    #print(w[0])\n",
    "    v = [x[1:199999] for x in w] # here is where you can edit embedding vocab size \n",
    "    w = [x[0] for x in w]\n",
    "    print(\"Word embedding vocab size: \",len(v)-1,'\\n')\n",
    "    \n",
    "\n",
    "    for i in range(len(w)):\n",
    "      words[w[i]] = v[i]\n",
    "\n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_embedding(vocab_size,itw):\n",
    "    print(\"Building word embedding matrix...\")\n",
    "    E = [None] * vocab_size\n",
    "    embedding_dict = load_embeddings()\n",
    "    for i in range(vocab_size): E[i] = embedding_dict[itw[i]]\n",
    "    return E\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cos_similarity(x,y):\n",
    "    x = np.array(x).astype(np.float)\n",
    "    y = np.array(y).astype(np.float)\n",
    "    \n",
    "    \n",
    "    d = np.dot(x,y) / (np.sqrt(np.dot(x,x))*(np.sqrt(np.dot(y,y))))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_similar_words(word,word_list,embeddings,similarity_threshold):\n",
    "    similarity = [cos_similarity(embeddings[word],embeddings[w[0]]) for w in word_list]\n",
    "    similarity = np.array(similarity)\n",
    "    if similarity[similarity.argmax()] > similarity_threshold:\n",
    "        return word,similarity.argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and format training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Function may vary based on training data file format.\n",
    "\n",
    "We format and tokenize our training data, then make a list of the most frequent words. Here we choose the vocab size our model will know. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename=\"../score/data.txt\", vocabulary_size=2000, min_sent_characters=10):\n",
    "    word_to_index = []\n",
    "    index_to_word = []\n",
    "    print(\"Reading text file...\")\n",
    "    with open(filename, 'rt') as f:\n",
    "        data = [line.split('|') for line in f.readlines()]\n",
    "        print(\"Raw training data: \",data[0][0])\n",
    "        #txt = [sent[0] for sent in data]\n",
    "        txt = [\"%s 'sentence_end'\" % sent[0] for sent in data]\n",
    "        \n",
    "        \n",
    "    print(\"Parsed %d articles.\" % (len(txt)))\n",
    "    tokenized_sentences = [nltk.word_tokenize(line.lower()) for line in txt]\n",
    "    \n",
    "    # Filter Words.\n",
    "    print(\"Filtering words...\")\n",
    "    tokenized_sentences = [[w[1:] for w in line] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if not w==''] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '\\\\' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '*' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '[' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if ']' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '\"' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if \"'\" not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if \"`\" not in w] for line in tokenized_sentences]\n",
    "    \n",
    "    # Replace all numbers with num_token\n",
    "    for i,line in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w.isnumeric()==False else NUM_TOKEN for w in line]\n",
    "        \n",
    "    # Count word frequencies and build vocab\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    n_data_words = len(word_freq.items())\n",
    "    print(\"Found %d unique words tokens.\\n\" % n_data_words)\n",
    "\n",
    "    \n",
    "    embeddings = load_embeddings()\n",
    "    vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    vocab = [w for w in vocab if w[0] in embeddings]\n",
    "    vocabulary_size = len(vocab)\n",
    "    print(\"Found %d out of %d words in GloVe embeddings.\" % (vocabulary_size,n_data_words))\n",
    "    \n",
    "    \n",
    "    \n",
    "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "    print(\"The least frequent word in our vocablary is '%s' and appeared %d times in this dataset.\" \\\n",
    "          % (vocab[-1][0], vocab[-1][1]))\n",
    "    \n",
    "    \n",
    "    \n",
    "    # We take the [vocabulary_size] most frequent words and build our word embedding matrix (or lookup table for now).  \n",
    "    # Words in dataset are now either inside or outside embedding matrix.\n",
    "    inside_words = sorted(vocab[:vocabulary_size], key=operator.itemgetter(1))\n",
    "    outside_words = sorted(vocab[vocabulary_size:], key=operator.itemgetter(1))\n",
    "    \n",
    "    index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN,SENTENCE_END_TOKEN] + [x[0] for x in inside_words]\n",
    "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "    \n",
    "    similar_words = {}\n",
    "    for w in outside_words:\n",
    "        try: \n",
    "            similar_word,similar_index = find_similar_words(w[0],inside_words,embeddings,similarity_threshold)\n",
    "            print(\"Replacing %s with %s\" % (similar_word,inside_words[similar_index][0]))\n",
    "            similar_words[similar_word] = inside_words[similar_index]\n",
    "        except: None\n",
    "        for line in tokenized_sentences:\n",
    "            for x in line:\n",
    "                if x in similar_words: x = similar_words[x] \n",
    "                    \n",
    "    print('\\nLeast freq words in vocab: ')\n",
    "    print(inside_words[:25])\n",
    "    print('\\n')\n",
    "    \n",
    "    # Save word_to_index & index_to_word if model is to be loaded after training\n",
    "    np.savez('index',itw=index_to_word,wti=word_to_index)\n",
    "    \n",
    "    # Filter sentences \n",
    "    for i,line in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in line]\n",
    "    tokenized_sentences = [s for s in tokenized_sentences if (len(s) > 1)]\n",
    "    \n",
    "\n",
    "    print('Filtered training data:')\n",
    "    print(tokenized_sentences[0])\n",
    "    print('\\n')\n",
    "\n",
    "    # Build training data\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "    y_train = [line[1] for line in data]\n",
    "    y_train = [10000*float(s.replace('\\n','')) for s in y_train]\n",
    "    return X_train,y_train,word_to_index,index_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading text file...\n",
      "Raw training data:  ['polypeptide', 'multiple', 'mainly', 'removal', 'cardinality', 'guarantee', 'indefinite', 'establishment', 'or', 'against', 'synchronized', 'recognition', 'algorithms', 'to', 'allow', 'way', 'latency', 'mentioned', 'reported']\n",
      "Parsed 12200 articles.\n",
      "Filtering words...\n",
      "Found 5938 unique words tokens.\n",
      "\n",
      "Loading word embeddings...\n",
      "Word embedding vocab size:  400000 \n",
      "\n",
      "Found 5710 out of 5938 words in GloVe embeddings.\n",
      "Using vocabulary size 5710.\n",
      "The least frequent word in our vocablary is 'acoustic' and appeared 1 times in this dataset.\n",
      "\n",
      "Least freq words in vocab: \n",
      "[('throwing', 1), ('speakers', 1), ('rice', 1), ('rebate', 1), ('pdb', 1), ('pathogen', 1), ('parabolic', 1), ('oscillator', 1), ('layers', 1), ('horizontal', 1), ('dictionaries', 1), ('diary', 1), ('contigs', 1), ('consciously', 1), ('charity', 1), ('cam', 1), ('breakpoints', 1), ('biomedical', 1), ('bin', 1), ('betting', 1), ('acoustic', 1), ('wall', 2), ('vertebrate', 2), ('uncorrelated', 2), ('ue', 2)]\n",
      "\n",
      "\n",
      "Filtered training data:\n",
      "['polypeptide', 'multiple', 'mainly', 'removal', 'cardinality', 'guarantee', 'indefinite', 'establishment', 'or', 'against', 'synchronized', 'recognition', 'algorithms', 'to', 'allow', 'way', 'latency', 'mentioned', 'reported', 'sentence_end']\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "d = load_data('../score/data.txt',4000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
