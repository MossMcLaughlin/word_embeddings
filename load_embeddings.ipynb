{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 3/2017 | Moss McLaughlin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import csv\n",
    "import itertools\n",
    "import time\n",
    "import operator\n",
    "import io\n",
    "import array\n",
    "from datetime import datetime\n",
    "\n",
    "import numpy as np\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "SENTENCE_START_TOKEN = \"sentence_start\"\n",
    "SENTENCE_END_TOKEN = \"sentence_end\"\n",
    "UNKNOWN_TOKEN = \"<UNK>\"\n",
    "ARTICLE_END_TOKEN = \"</ARTICLE_END>\"\n",
    "NUM_TOKEN = \"<NUM>\"\n",
    "\n",
    "similarity_threshold = 0.95\n",
    "replace_similar_words = True\n",
    "\n",
    "embedding_file = 'glove.6B/glove.6B.50d.txt'\n",
    "data_file = '../txtgen/data/IMDB_Data.txt'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load Pre-trained GloVe emeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Load pre-trained GloVe word emeddings and build a dictionary.\n",
    "Dataset has vocabulary of 400,000.\n",
    "\n",
    "Example lines (500,501,502) of glove.6B.50d.txt (words are 50 dimensional vectors):\n",
    "\n",
    "college -1.2283 1.4176 -0.68625 -1.1615 -0.0019627 -0.52577 -1.5977 -0.25307 -0.21699 -0.56572 -0.14248 0.15765 0.18763 -1.3173 -0.53438 0.20573 -0.47985 1.0476 -0.5075 0.77326 0.39774 0.70346 -0.32232 0.60171 -0.13352 -1.842 -0.092598 -1.1288 -1.4859 -0.86235 2.5976 0.66146 -0.054094 -1.5755 1.1185 0.13911 -0.26915 0.57159 1.3328 -0.24861 -0.68554 -0.43956 -0.86744 0.95772 -0.67349 0.36049 0.61952 -0.26356 -0.3715 0.32093\n",
    "\n",
    "working 0.25792 -0.14413 -0.035634 -0.60551 0.11004 -0.058799 -1.2209 -0.031605 -0.023699 -0.37419 0.28924 0.12331 -0.31903 0.65017 0.28362 -0.20956 0.30423 0.75571 0.47964 -0.41976 0.68923 0.92026 0.070798 0.3948 0.24721 -1.4038 -0.14209 -0.6946 -0.035052 0.0041205 3.4024 0.036271 -0.58483 -0.72107 0.036996 0.33065 -0.27332 0.51897 0.3499 0.061199 -0.36178 -0.26534 0.4271 0.0081181 0.19844 -0.38564 -0.35535 0.032932 -0.50055 0.54358\n",
    "\n",
    "community 0.14774 0.62713 -0.81852 -0.16878 0.44055 -0.12515 -0.87369 -0.060084 0.48804 -0.23463 -0.017574 -0.70673 \n",
    "0.77392 -0.82772 -0.69719 0.076588 0.98306 0.58452 0.77501 0.30231 0.0059052 0.58345 -1.0178 1.1176 -0.13487 -1.2102 -0.095842 -0.70611 -0.54565 -0.12818 3.4346 0.12549 -0.35702 -1.3685 -0.32705 -0.25489 -0.52943 -0.90213 0.28179 0.22691 -0.47532 -0.51934 -0.32232 0.07629 -0.10132 0.47723 -0.83698 0.021588 -0.36972 0.14843\n",
    "\n",
    "The dataset is ordered from most to least frequent tokens.\n",
    "The three most frequent tokens are ['the'], [','], ['.'].  \n",
    "\n",
    "We can build a smaller dictionary to increase speed by only reading the first n lines of glove.6B.50d.txt   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "  print(\"Loading word embeddings...\")\n",
    "  with open(embedding_file) as f:\n",
    "    words = {}\n",
    "    w = [line.split(' ') for line in f]\n",
    "    #print(w[0])\n",
    "    v = [x[1:] for x in w[:200000]] # here is where you can edit embedding vocab size \n",
    "    w = [x[0] for x in w[:200000]]\n",
    "    print(\"Word embedding vocab size: \",len(v),'\\n')\n",
    "    \n",
    "\n",
    "    for i in range(len(w)):\n",
    "      words[w[i]] = v[i]\n",
    "\n",
    "    return(words)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def create_embedding(vocab_size,itw):\n",
    "    print(\"Building word embedding matrix...\")\n",
    "    E = [None] * vocab_size\n",
    "    embedding_dict = load_embeddings()\n",
    "    for i in range(vocab_size): E[i] = embedding_dict[itw[i]]\n",
    "    return E\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def cos_similarity(x,y):\n",
    "    x = np.array(x).astype(np.float)\n",
    "    y = np.array(y).astype(np.float)\n",
    "    d = np.dot(x,y) / (np.sqrt(np.dot(x,x))*(np.sqrt(np.dot(y,y))))\n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def find_similar_words(word,word_list,embeddings,similarity_threshold):\n",
    "    similarity = [cos_similarity(embeddings[word],embeddings[w[0]]) for w in word_list]\n",
    "    similarity = np.array(similarity)\n",
    "    if similarity[similarity.argmax()] > similarity_threshold:\n",
    "        return word,similarity.argmax()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Load and format training data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "We format, tokenize and filter our training data, then make a list of the most frequent words. Here we choose the vocab size our model will know.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def load_data(filename, vocabulary_size=2000):\n",
    "    word_to_index = []\n",
    "    index_to_word = []\n",
    "    print(\"Reading text file...\")\n",
    "    with open(filename, 'rt') as f:\n",
    "        txt = f.read()\n",
    "        txt = txt.split(ARTICLE_END_TOKEN)\n",
    "        txt = [line.split('.') for line in txt]\n",
    "        txt.pop()\n",
    "        txt.pop()\n",
    "        for line in txt: line.pop()\n",
    "        print('Raw training data:')\n",
    "        print(txt[0][:2])\n",
    "        print('\\n')\n",
    "        print(txt[-1][-2:])\n",
    "        print('\\n')\n",
    "\n",
    "        \n",
    "        \n",
    "    print(\"Parsed %d sentences.\\n\" % (np.sum([len(article) for article in txt])))\n",
    "    \n",
    "    print(\"Tokenizing sentences...\")\n",
    "    tokenized_sentences = [[nltk.word_tokenize(line.replace('<br /><br />',' ').lower()) for line in article] for article in txt]\n",
    "    print(\"Done.\\n\")\n",
    "    \n",
    "    \n",
    "    for i,article in enumerate(tokenized_sentences):\n",
    "        a = []\n",
    "        for sent in article: a += sent\n",
    "        tokenized_sentences[i] = a\n",
    "    \n",
    "    \n",
    "    \n",
    "    # Filter Words.\n",
    "    print(\"Filtering words...\")\n",
    "    tokenized_sentences = [[w for w in line if not w==''] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '\\\\' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '*' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '[' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if ']' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if '\"' not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if \"'\" not in w] for line in tokenized_sentences]\n",
    "    tokenized_sentences = [[w for w in line if \"`\" not in w] for line in tokenized_sentences]\n",
    "    \n",
    "    \n",
    "    # Replace all numbers with num_token\n",
    "    for i,line in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w.isnumeric()==False else NUM_TOKEN for w in line]\n",
    "    print(\"Done.\\n\")\n",
    "    \n",
    "    print(tokenized_sentences[:5])\n",
    "    \n",
    "    # Count word frequencies and build vocab\n",
    "    word_freq = nltk.FreqDist(itertools.chain(*tokenized_sentences))\n",
    "    n_data_words = len(word_freq.items())\n",
    "    print(\"Found %d unique words tokens.\\n\" % n_data_words)\n",
    "\n",
    "    \n",
    "    embeddings = load_embeddings()\n",
    "    vocab = sorted(word_freq.items(), key=lambda x: (x[1], x[0]), reverse=True)\n",
    "    \n",
    "    # Only consider words that are in GloVe embeddings and appear at least twice.\n",
    "    vocab = [w for w in vocab if w[0] in embeddings]\n",
    "    n_glove_words = len(vocab)\n",
    "    print(\"Found %d out of %d words in GloVe embeddings.\" % (n_glove_words,n_data_words))\n",
    "    vocab = [w for w in vocab if w[1] > 1]\n",
    "    \n",
    "    # We take the [vocabulary_size] most frequent words and build our word embedding matrix (or lookup table for now).  \n",
    "    # Words in dataset are now either inside or outside embedding matrix.\n",
    "    inside_words = sorted(vocab[:vocabulary_size], key=operator.itemgetter(1))\n",
    "    outside_words = sorted(vocab[vocabulary_size:], key=operator.itemgetter(1))\n",
    "    print(\"%d out of %d words appears more than once.\\n\" % (len(vocab),n_glove_words))\n",
    "    \n",
    "    index_to_word = [\"<MASK/>\", UNKNOWN_TOKEN,SENTENCE_END_TOKEN] + [x[0] for x in inside_words]\n",
    "    word_to_index = dict([(w, i) for i, w in enumerate(index_to_word)])\n",
    "    \n",
    "    print(\"Using vocabulary size %d.\" % vocabulary_size)\n",
    "    print(\"The least frequent word in our vocablary is '%s' and appeared %d times in this dataset.\\n\" \\\n",
    "          % (inside_words[1][0], inside_words[1][1]))\n",
    "    \n",
    "    # Find similar words that are in the data set but outside of our vocabulary\n",
    "    if replace_similar_words:\n",
    "        print(\"Searching for similar words...\")\n",
    "        similar_words = {}\n",
    "        for w in outside_words:\n",
    "            try: \n",
    "                similar_word,similar_index = find_similar_words(w[0],inside_words,embeddings,similarity_threshold)\n",
    "                print(\"Replacing %s with %s\" % (similar_word,inside_words[similar_index][0]))\n",
    "                similar_words[similar_word] = inside_words[similar_index]\n",
    "            except: None\n",
    "            for line in tokenized_sentences:\n",
    "                for x in line:\n",
    "                    if x in similar_words: x = similar_words[x] \n",
    "                    \n",
    "    \n",
    "    # Save word_to_index & index_to_word if model is to be loaded after training\n",
    "    #np.savez('index',itw=index_to_word,wti=word_to_index)\n",
    "    \n",
    "    # Save vocab in a file with one words in each line, from most to least frequent \n",
    "    #         (if same vocab is to be used for training and later evaluation)\n",
    "    \n",
    "    # Filter sentences \n",
    "    for i,line in enumerate(tokenized_sentences):\n",
    "        tokenized_sentences[i] = [w if w in word_to_index else UNKNOWN_TOKEN for w in line]\n",
    "    tokenized_sentences = [s for s in tokenized_sentences if (len(s) > 1)]\n",
    "    \n",
    "\n",
    "    print('Filtered training data:')\n",
    "    print(tokenized_sentences[:5])\n",
    "    print('\\n')\n",
    "\n",
    "    # Build training data\n",
    "    X_train = np.asarray([[word_to_index[w] for w in sent[:-1]] for sent in tokenized_sentences])\n",
    "    y_train = np.asarray([[word_to_index[w] for w in sent[1:]] for sent in tokenized_sentences])\n",
    "    return X_train,y_train,word_to_index,index_to_word\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "d = load_data(data_file,6000)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
